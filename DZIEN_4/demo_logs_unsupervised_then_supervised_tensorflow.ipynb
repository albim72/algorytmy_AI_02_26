{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Logs ➜ (1) Unsupervised clustering ➜ (2) Supervised classification (TensorFlow)\n\nThis notebook is a cell-based version of the provided demo script. It:\n1. Loads log lines.\n2. Vectorizes them with `TextVectorization` (TF-IDF).\n3. Clusters with a minimal KMeans implemented in pure TensorFlow.\n4. Trains an MLP classifier to predict the discovered cluster id for new lines.\n\n> Tip: Keep epochs low for a live demo, then crank them up for better accuracy.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# If needed (e.g., in a fresh environment), uncomment and run:\n# !pip install -U tensorflow numpy\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from __future__ import annotations\n\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Helper functions"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def load_lines(path: Path) -> list[str]:\n    return path.read_text(encoding=\"utf-8\").splitlines()\n\n\ndef tf_kmeans(x: tf.Tensor, k: int, iters: int = 20, seed: int = 42) -> tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Minimal KMeans in TensorFlow.\n\n    Parameters\n    ----------\n    x:\n        Float tensor [N, D]\n    k:\n        Number of clusters\n    iters:\n        Iterations\n    seed:\n        Random seed\n\n    Returns\n    -------\n    labels:\n        int32 tensor [N]\n    centroids:\n        float32 tensor [K, D]\n    \"\"\"\n    tf.random.set_seed(seed)\n    n = tf.shape(x)[0]\n\n    # Init centroids by sampling points\n    idx = tf.random.shuffle(tf.range(n))[:k]\n    centroids = tf.gather(x, idx)\n\n    for _ in range(iters):\n        # distances: [N, K] using squared euclidean distance\n        x2 = tf.reduce_sum(tf.square(x), axis=1, keepdims=True)              # [N, 1]\n        c2 = tf.reduce_sum(tf.square(centroids), axis=1, keepdims=True)      # [K, 1]\n        xc = tf.matmul(x, centroids, transpose_b=True)                       # [N, K]\n        dists = x2 + tf.transpose(c2) - 2.0 * xc\n\n        labels = tf.cast(tf.argmin(dists, axis=1), tf.int32)                 # [N]\n\n        # recompute centroids\n        new_centroids = []\n        for j in range(k):\n            mask = tf.equal(labels, j)\n            pts = tf.boolean_mask(x, mask)\n            # handle empty cluster\n            new_c = tf.cond(\n                tf.shape(pts)[0] > 0,\n                lambda: tf.reduce_mean(pts, axis=0),\n                lambda: centroids[j],\n            )\n            new_centroids.append(new_c)\n        centroids = tf.stack(new_centroids, axis=0)\n\n    return labels, centroids\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Data: load your log file\n\nThe original script expects a file named:\n\n- `synthetic_system_logs.log`\n\nPlace it next to this notebook (same directory), or change the path below.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "log_path = Path(\"synthetic_system_logs.log\")\n\n# Basic sanity check\nif not log_path.exists():\n    raise FileNotFoundError(\n        f\"Missing {log_path}. Put your log file next to this notebook \"\n        \"or update `log_path`.\"\n    )\n\nlines = load_lines(log_path)\nprint(\"Loaded lines:\", len(lines))\nprint(\"First line:\", lines[0] if lines else \"<empty>\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Optional: quick synthetic log generator (only if you need a demo file)\n\nIf you don't have logs handy, you can generate a small synthetic file.\nRun this once, then re-run the data-loading cell above.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import random\nfrom datetime import datetime, timedelta\n\ndef gen_synthetic_logs(path: Path, n: int = 5000, seed: int = 42) -> None:\n    random.seed(seed)\n    start = datetime(2026, 2, 25, 10, 0, 0)\n\n    levels = [\"INFO\", \"WARN\", \"ERROR\"]\n    services = [\"gateway\", \"payments\", \"auth\", \"search\", \"profile\"]\n    regions = [\"eu-west-1\", \"eu-central-1\", \"us-east-1\"]\n    routes = [\"/v1/pay\", \"/v1/login\", \"/v1/search\", \"/v1/profile\", \"/v1/chargeback\"]\n    codes = [200, 201, 204, 400, 401, 403, 404, 429, 500, 502, 503, 504]\n\n    def pick_level(code):\n        if code >= 500: return \"ERROR\"\n        if code >= 400: return \"WARN\"\n        return \"INFO\"\n\n    lines = []\n    t = start\n    for i in range(n):\n        t = t + timedelta(seconds=random.randint(1, 7))\n        service = random.choice(services)\n        region = random.choice(regions)\n        route = random.choice(routes)\n\n        # Bias codes by service to create clusterable patterns\n        if service == \"payments\":\n            code = random.choices([200, 201, 429, 502, 504], weights=[55, 15, 10, 10, 10])[0]\n        elif service == \"auth\":\n            code = random.choices([200, 401, 403, 429], weights=[65, 20, 10, 5])[0]\n        elif service == \"gateway\":\n            code = random.choices([200, 502, 503, 504], weights=[70, 10, 10, 10])[0]\n        else:\n            code = random.choice(codes)\n\n        level = pick_level(code)\n        latency = max(5, int(random.gauss(120, 60)))\n        host = f\"srv-{random.randint(1, 8):02d}\"\n        trace = f\"{random.getrandbits(64):016x}\"\n        proto = random.choice([\"http/1.1\", \"http/2\"])\n\n        # Add a few recurring textual motifs to strengthen clustering\n        extra = \"\"\n        if code in (502, 503, 504):\n            extra = random.choice([\" upstream timeout\", \" bad gateway\", \" service unavailable\"])\n            latency = max(latency, random.randint(800, 2400))\n        if code == 429:\n            extra += \" rate_limited=true burst=ip\"\n        if code in (401, 403):\n            extra += \" user=anonymous token=missing\"\n\n        lines.append(\n            f\"{t.isoformat()}Z {level} service={service} host={host} region={region} \"\n            f\"route={route} latency_ms={latency} code={code} trace={trace} proto={proto}{extra}\"\n        )\n\n    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n\n# Uncomment to generate:\n# gen_synthetic_logs(Path(\"synthetic_system_logs.log\"), n=6000)\n# print(\"Generated synthetic_system_logs.log\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Vectorize logs ➜ TF-IDF\n\nWe use `TextVectorization(output_mode=\"tf-idf\")` with character-level 5-grams to avoid hand-written parsers.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "max_tokens = 8000\nvec = tf.keras.layers.TextVectorization(\n    standardize=None,\n    split=\"character\",\n    ngrams=5,\n    max_tokens=max_tokens,\n    output_mode=\"tf-idf\",\n)\n\ntext_ds = tf.data.Dataset.from_tensor_slices(lines).batch(256)\nvec.adapt(text_ds)\n\nX = vec(tf.constant(lines))      # [N, V] dense\nX = tf.cast(X, tf.float32)\n\n# Optional: L2 normalize (often helps KMeans)\nX = tf.linalg.l2_normalize(X, axis=1)\n\nprint(\"X shape:\", X.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Unsupervised clustering (KMeans in TensorFlow)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "k = 5\nlabels, centroids = tf_kmeans(X, k=k, iters=25, seed=42)\nlabels_np = labels.numpy()\n\nprint(\"Cluster counts:\")\nfor cid in range(k):\n    print(cid, int((labels_np == cid).sum()))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Print a few examples per cluster (for naming)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "examples = defaultdict(list)\nfor line, cid in zip(lines, labels_np):\n    if len(examples[int(cid)]) < 5:\n        examples[int(cid)].append(line)\n\nprint(\"\\n--- Cluster samples (for naming) ---\")\nfor cid in range(k):\n    print(f\"\\n[Cluster {cid}]\")\n    for ex in examples[cid]:\n        print(\"  \", ex)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Supervised model: MLP predicts cluster id\n\nWe turn the cluster assignments into labels, then train a small MLP classifier.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "y = tf.keras.utils.to_categorical(labels_np, num_classes=k)\n\n# split\nn = len(lines)\nidx = np.arange(n)\nrng = np.random.default_rng(42)\nrng.shuffle(idx)\ncut = int(n * 0.8)\ntr, te = idx[:cut], idx[cut:]\n\nX_tr, X_te = tf.gather(X, tr), tf.gather(X, te)\ny_tr, y_te = y[tr], y[te]\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(X.shape[1],)),\n    tf.keras.layers.Dense(256, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dense(k, activation=\"softmax\"),\n])\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(\n    X_tr, y_tr,\n    validation_split=0.15,\n    epochs=6,          # demo-speed; increase to 15-30 if you want\n    batch_size=128,\n    verbose=2\n)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Evaluate and predict a new line"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "loss, acc = model.evaluate(X_te, y_te, verbose=0)\nprint(f\"Test accuracy (predicting cluster id): {acc:.4f}\")\n\nnew_line = \"2026-02-25T12:13:01Z ERROR service=gateway host=srv-02 region=eu-west-1 upstream timeout service=payments route=/v1/pay latency_ms=1800 code=504 trace=deadbeef proto=http/2\"\nnew_vec = vec(tf.constant([new_line]))\nnew_vec = tf.linalg.l2_normalize(tf.cast(new_vec, tf.float32), axis=1)\npred_cluster = int(tf.argmax(model.predict(new_vec, verbose=0), axis=1).numpy()[0])\n\nprint(\"\\nNew line predicted cluster:\", pred_cluster)\nprint(\"Line:\", new_line)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---  \n### Notes for live training\n\n- **Cluster meaning**: clusters are discovered from text patterns, so you can \"name\" them by inspecting sample lines.\n- **Accuracy** here is \"how well the MLP reproduces KMeans labels\" (not ground truth). It's a proxy for whether clusters are learnable and stable.\n- For speed: reduce `max_tokens`, `k`, `iters`, or `epochs`.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}