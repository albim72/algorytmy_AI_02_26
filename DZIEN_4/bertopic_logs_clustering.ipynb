{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BERTopic for log clustering (human-label friendly)\n\nThis notebook shows a **BERTopic-style** workflow for logs:\n\n1. Load (or generate) logs  \n2. Normalize noisy tokens (timestamps, ids, IPs, numbers)  \n3. Run **BERTopic** (HDBSCAN + c-TF-IDF)  \n4. Inspect **topic keywords + representative log lines** so clusters are easy to label\n\n> Note: In this runtime, `bertopic` may not be preinstalled. The notebook includes an automatic fallback (\"MiniTopic\") that still provides **clusters + top keywords + examples** using only `scikit-learn`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# If you're running this locally / on Colab, run this cell once:\n# !pip install -U bertopic sentence-transformers hdbscan umap-learn\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from __future__ import annotations\n\nimport re\nimport random\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.cluster import KMeans\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Stopwords tuned for normalized logs:\n# after normalization, placeholders like <NUM>/<TS>/<HEX> often become tokens like 'num', 'ts', 'hex'\nLOG_STOPWORDS = {\"num\", \"ts\", \"hex\", \"trace\", \"latency_ms\", \"host\"}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Load or generate log file"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def gen_synthetic_logs(path: Path, n: int = 6000, seed: int = 42) -> None:\n    random.seed(seed)\n    start = datetime(2026, 2, 25, 10, 0, 0)\n\n    services = [\"gateway\", \"payments\", \"auth\", \"search\", \"profile\"]\n    regions = [\"eu-west-1\", \"eu-central-1\", \"us-east-1\"]\n    routes = [\"/v1/pay\", \"/v1/login\", \"/v1/search\", \"/v1/profile\", \"/v1/chargeback\"]\n\n    def pick_level(code: int) -> str:\n        if code >= 500:\n            return \"ERROR\"\n        if code >= 400:\n            return \"WARN\"\n        return \"INFO\"\n\n    lines = []\n    t = start\n    for _ in range(n):\n        t = t + timedelta(seconds=random.randint(1, 7))\n        service = random.choice(services)\n        region = random.choice(regions)\n        route = random.choice(routes)\n\n        if service == \"payments\":\n            code = random.choices([200, 201, 429, 502, 504], weights=[55, 15, 10, 10, 10])[0]\n        elif service == \"auth\":\n            code = random.choices([200, 401, 403, 429], weights=[65, 20, 10, 5])[0]\n        elif service == \"gateway\":\n            code = random.choices([200, 502, 503, 504], weights=[70, 10, 10, 10])[0]\n        else:\n            code = random.choice([200, 201, 204, 400, 401, 403, 404, 429, 500, 502, 503, 504])\n\n        level = pick_level(code)\n        latency = max(5, int(random.gauss(120, 60)))\n        host = f\"srv-{random.randint(1, 8):02d}\"\n        trace = f\"{random.getrandbits(64):016x}\"\n        proto = random.choice([\"http/1.1\", \"http/2\"])\n\n        extra = \"\"\n        if code in (502, 503, 504):\n            extra = random.choice([\" upstream timeout\", \" bad gateway\", \" service unavailable\"])\n            latency = max(latency, random.randint(800, 2400))\n        if code == 429:\n            extra += \" rate_limited=true burst=ip\"\n        if code in (401, 403):\n            extra += \" user=anonymous token=missing\"\n\n        lines.append(\n            f\"{t.isoformat()}Z {level} service={service} host={host} region={region} \"\n            f\"route={route} latency_ms={latency} code={code} trace={trace} proto={proto}{extra}\"\n        )\n\n    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n\n\nlog_path = Path(\"synthetic_system_logs.log\")\n\n# If you already have your own file, just put it next to this notebook and skip generation.\nif not log_path.exists():\n    gen_synthetic_logs(log_path, n=6000, seed=42)\n    print(\"Generated:\", log_path)\n\nlines = log_path.read_text(encoding=\"utf-8\").splitlines()\nprint(\"Loaded lines:\", len(lines))\nprint(\"Example:\", lines[0])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Normalize logs for better topics"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "TS_RE = re.compile(r\"\\b\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z\\b\")\nHEX_RE = re.compile(r\"\\b[0-9a-f]{12,}\\b\", re.IGNORECASE)\nIP_RE = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\nNUM_RE = re.compile(r\"\\b\\d+\\b\")\n\n\ndef normalize_log(s: str) -> str:\n    s = TS_RE.sub(\"<TS>\", s)\n    s = IP_RE.sub(\"<IP>\", s)\n    s = HEX_RE.sub(\"<HEX>\", s)\n    s = NUM_RE.sub(\"<NUM>\", s)\n    return s\n\n\nnorm_lines = [normalize_log(x) for x in lines]\nprint(\"Before:\", lines[0])\nprint(\"After :\", norm_lines[0])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) BERTopic (preferred) + fallback"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def show_cluster_report(topic_ids: np.ndarray, docs: list[str], top_words: dict[int, list[str]], n_examples: int = 5) -> None:\n    examples = defaultdict(list)\n    for doc, tid in zip(docs, topic_ids):\n        if len(examples[int(tid)]) < n_examples:\n            examples[int(tid)].append(doc)\n\n    unique = sorted(set(int(x) for x in topic_ids))\n    for tid in unique:\n        print(\"\\n\" + \"=\"*80)\n        print(f\"TOPIC/CLUSTER {tid} | count={(topic_ids==tid).sum()}\")\n        if tid in top_words:\n            print(\"keywords:\", \", \".join(top_words[tid]))\n        for ex in examples[tid]:\n            print(\" -\", ex[:220])\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "use_bertopic = False\ntry:\n    from bertopic import BERTopic\n    from sentence_transformers import SentenceTransformer\n    use_bertopic = True\nexcept Exception as e:\n    print(\"BERTopic not available here, using fallback. Reason:\", e)\n\nif use_bertopic:\n    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n    topic_model = BERTopic(\n        embedding_model=embedding_model,\n        language=\"english\",\n        verbose=True,\n        nr_topics=\"auto\",\n        min_topic_size=25,\n    )\n\n    topics, _ = topic_model.fit_transform(norm_lines)\n\n    # top keywords per topic\n    top_words = {}\n    for tid in sorted(set(topics)):\n        if tid == -1:\n            continue\n        words_scores = topic_model.get_topic(tid) or []\n        top_words[int(tid)] = [w for w, _ in words_scores[:10]]\n\n    show_cluster_report(np.array(topics), lines, top_words, n_examples=5)\n\nelse:\n    # MiniTopic fallback: KMeans + class-based TF-IDF keywords\n    n_clusters = 6\n    tfidf = TfidfVectorizer(\n        ngram_range=(1, 2),\n        min_df=2,\n        max_df=0.9,\n        stop_words=LOG_STOPWORDS,\n        token_pattern=r\"(?u)\\b[\\w=/\\.-]+\\b\",\n    )\n    X = tfidf.fit_transform(norm_lines)\n    km = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n    cluster_ids = km.fit_predict(X)\n\n    clusters_as_docs = []\n    for c in range(n_clusters):\n        joined = \" \".join([norm_lines[i] for i in range(len(norm_lines)) if cluster_ids[i] == c])\n        clusters_as_docs.append(joined)\n\n    cv = CountVectorizer(\n        ngram_range=(1, 2),\n        min_df=2,\n        stop_words=LOG_STOPWORDS,\n        token_pattern=r\"(?u)\\b[\\w=/\\.-]+\\b\",\n    )\n    counts = cv.fit_transform(clusters_as_docs).tocsr()\n    vocab = np.array(cv.get_feature_names_out())\n\n    tf = counts.multiply(1.0 / (counts.sum(axis=1) + 1e-9))\n    df = np.asarray((counts > 0).sum(axis=0)).ravel()\n    idf = np.log((1 + n_clusters) / (1 + df)) + 1\n    ctfidf = tf.multiply(idf)\n\n    top_words = {}\n    for c in range(n_clusters):\n        row = ctfidf.getrow(c).toarray().ravel()\n        top_idx = row.argsort()[::-1][:12]\n        top_words[c] = [vocab[i] for i in top_idx if row[i] > 0][:10]\n\n    show_cluster_report(np.array(cluster_ids), lines, top_words, n_examples=5)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Notes\n\nFor the most labelable topics:\n- keep normalization ON\n- increase `min_topic_size` (BERTopic)\n- consider adding domain stopwords (e.g., `proto`, `http/1.1`, etc.) if they dominate.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}